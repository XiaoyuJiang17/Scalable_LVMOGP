{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/')\n",
    "from code_blocks.kernels.periodic_inputs_maternkernel import PeriodicInputsMaternKernel\n",
    "from code_blocks.likelihoods.gaussian_likelihood import GaussianLikelihood\n",
    "from gpytorch.kernels import MaternKernel\n",
    "import torch\n",
    "\n",
    "original_kernel = MaternKernel(nu=2.5)\n",
    "test_kernel = PeriodicInputsMaternKernel(nu=2.5, period=0.33)\n",
    "test_likelihood = GaussianLikelihood()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_tensor = torch.tensor([0.5, 1, 2, 3])\n",
    "test_kernel(test_tensor).to_dense()\n",
    "x1 = 2 * test_tensor * torch.pi / 0.33\n",
    "x1 = torch.cat((torch.sin(x1).reshape(-1, 1), torch.cos(x1).reshape(-1, 1)), axis=1)\n",
    "original_kernel(x1).to_dense()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "original_kernel.lengthscale = 15.\n",
    "for name, params in original_kernel.named_parameters():\n",
    "    print(name)\n",
    "    print(params)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import yaml\n",
    "\n",
    "root_config = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/configs/' \n",
    "# NOTE: Specify name here for different experiments: \n",
    "# rnd (fix) + unfix (fix) ; first referring to initialization, second referring to inducing points in input space \n",
    "curr_config_name = 'spatiotemp/Scale_Matern52_plus_Scale_PeriodicInputsMatern52/lvmogp_catlatent_rnd_unfix' \n",
    "curr_config = f'{root_config}/{curr_config_name}.yaml'\n",
    "with open(curr_config, 'r') as file: \n",
    "    config = yaml.safe_load(file) \n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/')\n",
    "from code_blocks.likelihoods.gaussian_likelihood import GaussianLikelihood\n",
    "from gplvm_init import GPLVM, train_gplvm\n",
    "from run_experiments.prepare_dataset import prepare_spatio_temp_data\n",
    "import torch\n",
    "\n",
    "\n",
    "_, data_Y_squeezed, *arg = prepare_spatio_temp_data(config=config)\n",
    "\n",
    "data_Y = data_Y_squeezed.reshape(config['n_outputs'], config['n_input'])[:, :config['n_input_train']]\n",
    "\n",
    "class GaussianLikelihoodWithMissingObs(GaussianLikelihood):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_masked_obs(x):\n",
    "        missing_idx = x.isnan()\n",
    "        x_masked = x.masked_fill(missing_idx, -999.)\n",
    "        return missing_idx, x_masked\n",
    "\n",
    "    def expected_log_prob(self, target, input, *params, **kwargs):\n",
    "        missing_idx, target = self._get_masked_obs(target)\n",
    "        res = super().expected_log_prob(target, input, *params, **kwargs)\n",
    "        return res * ~missing_idx\n",
    "\n",
    "    def log_marginal(self, observations, function_dist, *params, **kwargs):\n",
    "        missing_idx, observations = self._get_masked_obs(observations)\n",
    "        res = super().log_marginal(observations, function_dist, *params, **kwargs)\n",
    "        return res * ~missing_idx\n",
    "    \n",
    "my_GPLVM = GPLVM(n = config['n_outputs'], \n",
    "                 data_dim= config['n_input_train'], \n",
    "                 latent_dim = 2, \n",
    "                 n_inducing = 20)\n",
    "\n",
    "my_likelihood = GaussianLikelihoodWithMissingObs()\n",
    "\n",
    "_, _, losses = train_gplvm(my_GPLVM, my_likelihood, data_Y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cached function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpytorch.utils.memoize import cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class class_test():\n",
    "\n",
    "    def __init__(self, a=1.):\n",
    "        self.a = a\n",
    "    \n",
    "    # @property\n",
    "    @cached(name='value_b')\n",
    "    def value_b(self):\n",
    "        return self.a ** 2\n",
    "\n",
    "class_test = class_test(a=10.)\n",
    "print(class_test.value_b())\n",
    "print(class_test._memoize_cache)\n",
    "class_test.a = 11.\n",
    "print(class_test.a)\n",
    "print(class_test.value_b())\n",
    "print(class_test._memoize_cache)\n",
    "\n",
    "class class_test_2():\n",
    "\n",
    "    def __init__(self, a=1.):\n",
    "        self.a = a\n",
    "    \n",
    "    # @property\n",
    "    @cached(name='value_b')\n",
    "    def value_b(self, c):\n",
    "        return self.a ** c\n",
    "    \n",
    "class_test_2 = class_test_2(a=10.)\n",
    "print(class_test_2.value_b(c=2.))\n",
    "print(class_test_2._memoize_cache)\n",
    "\n",
    "class_test_2.a = 11.\n",
    "\n",
    "print(class_test_2.value_b(c=2.))\n",
    "print(class_test_2._memoize_cache)\n",
    "\n",
    "print(class_test_2.value_b(c=3.))\n",
    "print(class_test_2._memoize_cache)\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dumps({'c': 2.})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KL cmputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import Tensor\n",
    "from linear_operator.operators import CholLinearOperator, TriangularLinearOperator, KroneckerProductLinearOperator, DiagLinearOperator\n",
    "import time\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.distributions import MultivariateNormal as TMultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_kronecker_wrt_identity_1(chol_variational_covar_latent: Tensor, \n",
    "                                           chol_variational_covar_input: Tensor, \n",
    "                                           variational_mean: Tensor):\n",
    "    \n",
    "    M_X, M_H = chol_variational_covar_input.shape[0], chol_variational_covar_latent.shape[0]\n",
    "    _variational_mean = variational_mean.reshape(M_H, M_X)\n",
    "    # log_det_varcov_latent = (torch.prod(torch.diag(chol_variational_covar_latent)) ** 2).log()\n",
    "    # log_det_varcov_input = (torch.prod(torch.diag(chol_variational_covar_input)) ** 2).log()\n",
    "    log_det_varcov_latent = 2 * torch.sum(torch.log(torch.diag(chol_variational_covar_latent)))\n",
    "    log_det_varcov_input = 2 * torch.sum(torch.log(torch.diag(chol_variational_covar_input)))\n",
    "    tr_varcov_latent = torch.sum(chol_variational_covar_latent ** 2)\n",
    "    tr_carcov_input = torch.sum(chol_variational_covar_input ** 2)\n",
    "    tr_MTM = torch.norm(_variational_mean, p='fro') ** 2 # trace(M^TM) = F_norm(M)^2\n",
    "    \n",
    "    res = - M_X * log_det_varcov_latent - M_H * log_det_varcov_input + tr_MTM + tr_carcov_input*tr_varcov_latent - M_X*M_H \n",
    "    return res/2\n",
    "\n",
    "\n",
    "\n",
    "def kl_divergence_kronecker_wrt_identity_2(chol_variational_covar_latent: Tensor, \n",
    "                                           chol_variational_covar_input: Tensor, \n",
    "                                           variational_mean: Tensor):\n",
    "    '''KL divergence of q and p, where q has kronecker product covariance and p is standard normal distribution.'''\n",
    "    \n",
    "    M_X, M_H = chol_variational_covar_input.shape[0], chol_variational_covar_latent.shape[0]\n",
    "    _variational_mean = variational_mean.reshape(M_H, M_X)\n",
    "    log_det_varcov_latent = (torch.prod(torch.diag(chol_variational_covar_latent)) ** 2).log()\n",
    "    log_det_varcov_input = (torch.prod(torch.diag(chol_variational_covar_input)) ** 2).log()\n",
    "    tr_varcov_latent = torch.sum(chol_variational_covar_latent ** 2)\n",
    "    tr_carcov_input = torch.sum(chol_variational_covar_input ** 2)\n",
    "    tr_MTM = torch.norm(_variational_mean, p='fro') ** 2 # trace(M^TM) = F_norm(M)^2\n",
    "    \n",
    "    res = - M_X * log_det_varcov_latent - M_H * log_det_varcov_input + tr_MTM + tr_carcov_input*tr_varcov_latent - M_X*M_H \n",
    "    return res/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_H = 40\n",
    "M_X = 50\n",
    "L_H_1 = torch.randn(M_H, M_H)\n",
    "L_X_1 = torch.randn(M_X, M_X)\n",
    "# _L_H = torch.eye(M_H).to(torch.double)\n",
    "# _L_X = torch.eye(M_X).to(torch.double)\n",
    "variational_mean = torch.randn(M_X * M_H)\n",
    "# variational_mean[:10] = torch.pi\n",
    "\n",
    "lower_mask_latent = torch.ones(L_H_1.shape[-2:]).tril(0)\n",
    "lower_mask_input = torch.ones(L_X_1.shape[-2:]).tril(0)\n",
    "L_H = L_H_1.mul(lower_mask_latent)\n",
    "L_X = L_X_1.mul(lower_mask_input)\n",
    "\n",
    "for i in range(L_H.size(0)): \n",
    "    L_H[i, i] = L_H[i, i] ** 2 + 1.\n",
    "\n",
    "\n",
    "for i in range(L_X.size(0)): \n",
    "    L_X[i, i] = L_X[i, i] ** 2 + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varcov = KroneckerProductLinearOperator(CholLinearOperator(TriangularLinearOperator(L_H)), CholLinearOperator(TriangularLinearOperator(L_X))).to_dense()\n",
    "# varcov_ = torch.kron(CholLinearOperator(TriangularLinearOperator(L_H)).to_dense(), CholLinearOperator(TriangularLinearOperator(L_X)).to_dense())\n",
    "\n",
    "# (varcov_ - varcov).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varcov = KroneckerProductLinearOperator(CholLinearOperator(TriangularLinearOperator(L_H)), CholLinearOperator(TriangularLinearOperator(L_X)))\n",
    "# varcov_ = torch.kron(CholLinearOperator(TriangularLinearOperator(L_H)).to_dense(), CholLinearOperator(TriangularLinearOperator(L_X)).to_dense())\n",
    "q_u = MultivariateNormal(variational_mean, varcov)\n",
    "zeros = torch.zeros(variational_mean.shape[0])\n",
    "p_u = MultivariateNormal(zeros, DiagLinearOperator(torch.ones_like(zeros)))\n",
    "\n",
    "q_u_2 = TMultivariateNormal(variational_mean, varcov.to_dense())\n",
    "p_u_2 = TMultivariateNormal(zeros, torch.eye(zeros.shape[0]))\n",
    "\n",
    "# make sure diag all positive ... \n",
    "start_time = time.time()\n",
    "L_H_ = torch.linalg.cholesky(CholLinearOperator(TriangularLinearOperator(L_H)).to_dense())\n",
    "L_X_ = torch.linalg.cholesky(CholLinearOperator(TriangularLinearOperator(L_X)).to_dense())\n",
    "end_time = time.time()\n",
    "\n",
    "print('total time', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(q_u.covariance_matrix == q_u_2.covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 questions:\n",
    "1. why varcov.to_dense() not psd? It should be. Especially when matrix goes large ... \n",
    "2. why Gpytorch based MultivariateNormal havn't report this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_variational_mean = variational_mean.reshape(M_H, M_X)\n",
    "true_KL = 0.5 * (torch.diag(_variational_mean @ _variational_mean.T).sum())\n",
    "print('true_KL', true_KL)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "res_1 = kl_divergence_kronecker_wrt_identity_2(L_H_, L_X_, variational_mean)\n",
    "end_time = time.time()\n",
    "print('Our method log sum', res_1)\n",
    "print('total time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "res_1 = kl_divergence_kronecker_wrt_identity_1(L_H_, L_X_, variational_mean)\n",
    "end_time = time.time()\n",
    "print('Our method prod log', res_1)\n",
    "print('total time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "res_2 = torch.distributions.kl.kl_divergence(q_u, p_u)\n",
    "end_time = time.time()\n",
    "print('GpyTorch method', res_2)\n",
    "print('total time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "res_2 = torch.distributions.kl.kl_divergence(q_u_2, p_u_2)\n",
    "end_time = time.time()\n",
    "print('torch method', res_2)\n",
    "print('total time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _batch_trace_XXT(bmat):\n",
    "    \"\"\"\n",
    "    Utility function for calculating the trace of XX^{T} with X having arbitrary trailing batch dimensions\n",
    "    \"\"\"\n",
    "    n = bmat.size(-1)\n",
    "    m = bmat.size(-2)\n",
    "    flat_trace = bmat.reshape(-1, m * n).pow(2).sum(-1)\n",
    "    return flat_trace.reshape(bmat.shape[:-2])\n",
    "\n",
    "from torch.distributions.multivariate_normal import _batch_mahalanobis\n",
    "\n",
    "def _kl_multivariatenormal_multivariatenormal(p, q):\n",
    "    # From https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence\n",
    "    if p.event_shape != q.event_shape:\n",
    "        raise ValueError(\"KL-divergence between two Multivariate Normals with\\\n",
    "                          different event shapes cannot be computed\")\n",
    "\n",
    "    # why p._unbroadcasted_scale_tril returns negative values ?\n",
    "    half_term1 = (q._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1) -\n",
    "                  torch.linalg.cholesky(q_u.covariance_matrix).diag().log().sum(-1))\n",
    "    \n",
    "    combined_batch_shape = torch._C._infer_size(q._unbroadcasted_scale_tril.shape[:-2],\n",
    "                                                p._unbroadcasted_scale_tril.shape[:-2])\n",
    "    n = p.event_shape[0]\n",
    "    q_scale_tril = q._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))\n",
    "    p_scale_tril = torch.linalg.cholesky(q_u.covariance_matrix).expand(combined_batch_shape + (n, n))\n",
    "    term2 = _batch_trace_XXT(torch.linalg.solve_triangular(q_scale_tril, p_scale_tril, upper=False))\n",
    "    term3 = _batch_mahalanobis(q._unbroadcasted_scale_tril, (q.loc - p.loc))\n",
    "    return half_term1 + 0.5 * (term2 + term3 - n)\n",
    "\n",
    "start_time = time.time()\n",
    "res_2 = _kl_multivariatenormal_multivariatenormal(q_u, p_u)\n",
    "end_time = time.time()\n",
    "print(res_2)\n",
    "print('total time:', end_time - start_time)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KL between latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mu = torch.zeros(100, 2)\n",
    "q_sigma = torch.ones(100, 2) + 1.\n",
    "\n",
    "batch_idx = [i for i in range(10)]\n",
    "\n",
    "q_mu_batch = q_mu[batch_idx, ...]\n",
    "q_sigma_batch = q_sigma[batch_idx, ...]\n",
    "\n",
    "# --- --- --- ---\n",
    "p_mu = torch.zeros(100, 2)\n",
    "p_sigma = torch.ones(100, 2)\n",
    "\n",
    "p_mu_batch = p_mu[batch_idx, ...]\n",
    "p_sigma_batch = p_sigma[batch_idx, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_x = torch.distributions.Normal(q_mu_batch, q_sigma_batch)\n",
    "p_x = torch.distributions.Normal(p_mu_batch, p_sigma_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributions.kl_divergence(q_x, p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test elementwise product with linear operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from linear_operator.operators import KroneckerProductLinearOperator, TriangularLinearOperator, LinearOperator, CholLinearOperator, DenseLinearOperator, MatmulLinearOperator, SumLinearOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.eye(4) * torch.tensor([1, 2, 4, 5])\n",
    "B = torch.eye(4) * torch.rand(4)\n",
    "C = torch.tensor([[10., 0.], [0., 30.]])\n",
    "D = torch.tensor([[15., 0.], [0., 70.]])\n",
    "E = torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AmulB_lt = SumLinearOperator(A, B)\n",
    "C_lt = KroneckerProductLinearOperator(C, D)\n",
    "E_lt = DenseLinearOperator(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  205.1507,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,  1533.7067,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,  1840.7195,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000, 11895.7988]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AmulB_lt.mul(C_lt).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  205.1507,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,  1533.7070,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,  1840.7194,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000, 11895.7988]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( A + B ).mul(torch.kron(C, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( A + (B) ) * torch.kron(C, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
