{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP')\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from run_experiments.prepare_dataset import *\n",
    "from utils_general import prepare_common_background_info, pred4all_outputs_inputs, evaluate_on_single_output, plot_traindata_testdata_fittedgp, neg_log_likelihood\n",
    "from code_blocks.our_models.lvmogp_svi import LVMOGP_SVI\n",
    "from code_blocks.likelihoods.gaussian_likelihood import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/configs/synthetic/Scale_RBF/lvmogp_50_outputs_unfix.yaml'\n",
    "model_path = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/experiments_results/synthetic/Scale_RBF/lvmogp_50_outputs_unfix/2024-03-02_17:56:36/model.pth'\n",
    "likelihood_path = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/experiments_results/synthetic/Scale_RBF/lvmogp_50_outputs_unfix/2024-03-02_17:56:36/likelihood.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_name) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "my_model = LVMOGP_SVI(\n",
    "        n_outputs = config['n_outputs'],\n",
    "        n_input = config['n_input_train'],                    # NOTE PAY ATTENTION, not total n_inputs.\n",
    "        input_dim = config['input_dim'],\n",
    "        latent_dim = config['latent_dim'],\n",
    "        n_inducing_input = config['n_inducing_input'],\n",
    "        n_inducing_latent = config['n_inducing_latent'],\n",
    "        learn_inducing_locations_latent = config['learn_inducing_locations_latent'],\n",
    "        learn_inducing_locations_input = config['learn_inducing_locations_input'],\n",
    "        latent_kernel_type = config['latent_kernel_type'],\n",
    "        input_kernel_type = config['input_kernel_type'],\n",
    "        trainable_latent_dim = config['trainable_latent_dim'] if 'trainable_latent_dim' in config else None,\n",
    "        latent_first_init = None,                               # if None, random initialization\n",
    "        latent_second_init = None,                              # if None, random initialization\n",
    "        NNEncoder = config['NNEncoder'],\n",
    "        layers = None                                           # if none, adopt default value [4, 8, 4]\n",
    "    )   \n",
    "\n",
    "my_likelihood = GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(model_path)\n",
    "my_model.load_state_dict(model_state_dict)\n",
    "\n",
    "likelihood_state_dict = torch.load(likelihood_path)\n",
    "my_likelihood.load_state_dict(likelihood_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from linear_operator.operators import KroneckerProductLinearOperator, TriangularLinearOperator, LinearOperator, CholLinearOperator\n",
    "\n",
    "chol_covar_latent_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_latent.data\n",
    "covar_latent_u = CholLinearOperator(chol_covar_latent_u)\n",
    "chol_covar_input_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_input.data\n",
    "covar_input_u = CholLinearOperator(chol_covar_input_u)\n",
    "\n",
    "covar_u = KroneckerProductLinearOperator(covar_latent_u, covar_input_u)\n",
    "\n",
    "covar_u.to_dense()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset_type'] == 'synthetic_regression':\n",
    "        data_inputs, data_Y_squeezed, ls_of_ls_train_input, ls_of_ls_test_input, train_sample_idx_ls, test_sample_idx_ls = prepare_synthetic_regression_data(config)\n",
    "        means, stds = None, None\n",
    "        \n",
    "n_data4visual = 500\n",
    "inputs_total4visual = Tensor(np.linspace(config['min_input_bound'], config['max_input_bound'], n_data4visual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test via integration (datapoint by datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import CholLinearOperator, KroneckerProductLinearOperator\n",
    "from linear_operator.operators import (\n",
    "    LinearOperator,\n",
    "    TriangularLinearOperator,\n",
    ")\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.settings import _linalg_dtype_cholesky\n",
    "\n",
    "def prepare_common_background_info(my_model, config):\n",
    "    '''Prepare all values of a dict called common_background_information, which being used in integration_prediction_func'''\n",
    "    \n",
    "    def _cholesky_factor(induc_induc_covar: LinearOperator) -> TriangularLinearOperator:\n",
    "        L = psd_safe_cholesky(to_dense(induc_induc_covar).type(_linalg_dtype_cholesky.value()), max_tries=4)\n",
    "        return TriangularLinearOperator(L)\n",
    "    \n",
    "    K_uu_latent = my_model.covar_module_latent(my_model.variational_strategy.inducing_points_latent.data).to_dense().to(torch.float64)\n",
    "    # K_uu_latent_inv = torch.linalg.solve(K_uu_latent, torch.eye(K_uu_latent.size(-1)).to(torch.float64))\n",
    "    K_uu_input = my_model.covar_module_input(my_model.variational_strategy.inducing_points_input.data).to_dense().to(torch.float64)\n",
    "    # K_uu_input_inv = torch.linalg.solve(K_uu_input, torch.eye(K_uu_input.size(-1)).to(torch.float64))\n",
    "\n",
    "    K_uu = KroneckerProductLinearOperator(K_uu_latent, K_uu_input).to_dense().data\n",
    "    # chol_K_uu_inv_t = _cholesky_factor_latent(KroneckerProductLinearOperator(K_uu_latent_inv, K_uu_input_inv)).to_dense().data.t()\n",
    "    chol_K_uu_inv_t = KroneckerProductLinearOperator(\n",
    "            torch.linalg.solve( _cholesky_factor(K_uu_latent).to_dense().data, torch.eye(K_uu_latent.size(-1)).to(torch.float64)),\n",
    "            torch.linalg.solve( _cholesky_factor(K_uu_input).to_dense().data, torch.eye(K_uu_input.size(-1)).to(torch.float64)),\n",
    "        ).to_dense().data.t()\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    chol_covar_latent_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_latent.data.to(torch.float64)\n",
    "    covar_latent_u = CholLinearOperator(chol_covar_latent_u).to_dense()\n",
    "    chol_covar_input_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_input.data.to(torch.float64)\n",
    "    covar_input_u = CholLinearOperator(chol_covar_input_u).to_dense()\n",
    "\n",
    "    covar_u = KroneckerProductLinearOperator(covar_latent_u, covar_input_u).to_dense().data\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    common_background_information = {\n",
    "                        'K_uu': K_uu.data,\n",
    "                        'chol_K_uu_inv_t': chol_K_uu_inv_t.data, \n",
    "                        'm_u': my_model.variational_strategy._variational_distribution.variational_mean.data,\n",
    "                        'Sigma_u': covar_u.data,\n",
    "                        'A': chol_K_uu_inv_t @ (covar_u - torch.eye(covar_u.shape[0])) @ chol_K_uu_inv_t.t(),\n",
    "                        'var_H': my_model.covar_module_latent.outputscale.data,\n",
    "                        'var_X': my_model.covar_module_input.outputscale.data,\n",
    "                        'W': my_model.covar_module_latent.base_kernel.lengthscale.data.reshape(-1)**2\n",
    "                        }\n",
    "    '''\n",
    "    chol_K_uu_inv_t: inverse of K_uu matrix, of shape (M_H * M_X, M_H * M_X)\n",
    "    m_u: mean of the variational distribution\n",
    "    Sigma_u: covariance matrix of the variational distribution\n",
    "    A: chol_K_uu_inv_t (Sigma_u - K_uu) chol_K_uu_inv_t.T\n",
    "    var_H: \n",
    "    var_X: \n",
    "    W: vector; containing all lengthscales in the RAD kernel\n",
    "    c: constant\n",
    "    '''\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    c = (2 * torch.pi)**(config['latent_dim'] / 2) * common_background_information['var_H'] * common_background_information['W'].sqrt().prod()\n",
    "    common_background_information['constant_c'] = c\n",
    "\n",
    "    return common_background_information\n",
    "\n",
    "common_background_information = prepare_common_background_info(my_model, config)\n",
    "\n",
    "def integration_prediction_func(test_input, output_index, my_model, common_background_information=common_background_information):\n",
    "\n",
    "    input_K_f_u = my_model.covar_module_input(test_input, my_model.variational_strategy.inducing_points_input.data).to_dense().data\n",
    "    input_K_u_f_K_f_u = input_K_f_u.t() @ input_K_f_u\n",
    "\n",
    "    data_specific_background_information = {\n",
    "            'm_plus': my_model.X.q_mu.data[output_index],\n",
    "            'Sigma_plus': 1.0 * my_model.X.q_log_sigma.exp().square().data[output_index],\n",
    "            'input_K_f_u': input_K_f_u, \n",
    "            'input_K_u_f_K_f_u': input_K_u_f_K_f_u,\n",
    "            'expectation_K_uu': None\n",
    "    }\n",
    "    \n",
    "    # helper functions -----------------------------------------------------------------------------------------------------------------------\n",
    "    def multivariate_gaussian_pdf(x, mu, cov):\n",
    "        '''cov is a vector, representing all elements in the diagonal matrix'''\n",
    "        k = mu.size(0)\n",
    "        cov_det = cov.prod()\n",
    "        cov_inv = torch.diag(1.0 / cov)\n",
    "        norm_factor = torch.sqrt((2 * torch.pi) ** k * cov_det)\n",
    "\n",
    "        x_mu = x - mu\n",
    "        result = torch.exp(-0.5 * x_mu @ cov_inv @ x_mu.t()) / norm_factor\n",
    "        return result.item()\n",
    "\n",
    "    def G(h:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "\n",
    "        mu = data_specific_background_information['m_plus']\n",
    "        cov_diag = data_specific_background_information['Sigma_plus'] + common_background_information['W']\n",
    "        result = multivariate_gaussian_pdf(h, mu, cov_diag)\n",
    "        return common_background_information['constant_c'] * result\n",
    "\n",
    "    def R(h_1:Tensor, h_2:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        mu_1 = h_2\n",
    "        cov_diag_1 = 2 * common_background_information['W']\n",
    "        mu_2 = (h_1 + h_2) / 2\n",
    "        cov_diag_2 = 0.5 * common_background_information['W'] + data_specific_background_information['Sigma_plus']\n",
    "        result1 = multivariate_gaussian_pdf(h_1, mu_1, cov_diag_1)\n",
    "        result2 = multivariate_gaussian_pdf(data_specific_background_information['m_plus'], mu_2, cov_diag_2)\n",
    "        return (common_background_information['constant_c'] ** 2 ) * result1 * result2\n",
    "    \n",
    "    def expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_f_u'].reshape(1, -1), data_specific_background_information['input_K_f_u'].reshape(1, -1)).to_dense().data \n",
    "        result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u'].to(result_.dtype)\n",
    "        return result_\n",
    "        \n",
    "    def expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = common_background_information['m_u']\n",
    "        _result = result_ @ common_background_information['chol_K_uu_inv_t'].t().to(result_.dtype)\n",
    "        interm_term = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "        result_ = _result @ interm_term.to(result_.dtype) @ _result.t()\n",
    "        # result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u']\n",
    "\n",
    "        if data_specific_background_information['expectation_K_uu'] == None:\n",
    "            data_specific_background_information['expectation_K_uu'] = interm_term\n",
    "        return result_\n",
    "        \n",
    "    def expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = common_background_information['var_H'] * common_background_information['var_X']\n",
    "\n",
    "        if data_specific_background_information['expectation_K_uu'] == None:\n",
    "            data_specific_background_information['expectation_K_uu'] = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], \\\n",
    "                                                                                                    data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "        result = result_ + (common_background_information['A'] * data_specific_background_information['expectation_K_uu']).sum()\n",
    "        return result\n",
    "    \n",
    "    def integration_predictive_mean(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        return expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)\n",
    "\n",
    "\n",
    "    def integration_predictive_var(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        return expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "            + expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "            - expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)**2\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    expectation_latent_K_f_u = Tensor([G(my_model.variational_strategy.inducing_points_latent.data[i]).item() for i in range(config['n_inducing_latent'])])\n",
    "    expectation_latent_K_u_f_K_f_u = Tensor([R(my_model.variational_strategy.inducing_points_latent.data[i], my_model.variational_strategy.inducing_points_latent.data[j]).item() \\\n",
    "                                            for j in range(config['n_inducing_latent']) for i in range(config['n_inducing_latent'])]).reshape(config['n_inducing_latent'], config['n_inducing_latent'])\n",
    "\n",
    "    data_specific_background_information['expectation_latent_K_f_u'] = expectation_latent_K_f_u\n",
    "    data_specific_background_information['expectation_latent_K_u_f_K_f_u'] = expectation_latent_K_u_f_K_f_u\n",
    "    \n",
    "    return integration_predictive_mean(data_specific_background_information=data_specific_background_information), \\\n",
    "           integration_predictive_var(data_specific_background_information=data_specific_background_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.eval()\n",
    "my_likelihood.eval()\n",
    "\n",
    "all_index_latent = np.array([[i]*config['n_input'] for i in range(config['n_outputs'])]).reshape(-1).tolist() \n",
    "all_index_input = [i for i in range(config['n_input'])] * config['n_outputs'] \n",
    "len_outputs = len(all_index_latent)\n",
    "assert len_outputs == len(all_index_input)\n",
    "\n",
    "all_pred_mean_slow = torch.zeros(len_outputs)\n",
    "all_pred_var_slow = torch.zeros(len_outputs)\n",
    "\n",
    "# iteratively inference\n",
    "for idx in trange(len_outputs, leave=True):\n",
    "    curr_latent_index = all_index_latent[idx]\n",
    "    curr_input = data_inputs[all_index_input[idx]].reshape(-1)\n",
    "    curr_pred_mean, curr_pred_var = integration_prediction_func(test_input=curr_input,\n",
    "                                                                output_index=curr_latent_index,\n",
    "                                                                my_model=my_model,\n",
    "                                                                common_background_information=common_background_information)\n",
    "    all_pred_mean_slow[idx] = curr_pred_mean\n",
    "    all_pred_var_slow[idx] = curr_pred_var + my_likelihood.noise.data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predict = all_pred_mean_slow[train_sample_idx_ls]\n",
    "train_rmse = (train_data_predict - data_Y_squeezed[train_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Train RMSE via integration', train_rmse)\n",
    "\n",
    "w_test_data_predict = all_pred_mean_slow[test_sample_idx_ls]\n",
    "test_rmse = (w_test_data_predict - data_Y_squeezed[test_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Test RMSE via integration', test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nll = neg_log_likelihood(Target=data_Y_squeezed[train_sample_idx_ls], GaussianMean=all_pred_mean_slow[train_sample_idx_ls], GaussianVar=all_pred_var_slow[train_sample_idx_ls])\n",
    "test_nll = neg_log_likelihood(Target=data_Y_squeezed[test_sample_idx_ls], GaussianMean=all_pred_mean_slow[test_sample_idx_ls], GaussianVar=all_pred_var_slow[test_sample_idx_ls])\n",
    "\n",
    "print('Global Train negative log likelihood via integration:', train_nll)\n",
    "print('Global Test negative log likelihood via integration:', test_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test via integration (output by output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_background_information = prepare_common_background_info(my_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test something\n",
    "'''\n",
    "from utils_general import prepare_pre_computation, integration_prediction_func\n",
    "pre_compute_dict = prepare_pre_computation(my_model, data_inputs)\n",
    "curr_pred_mean, curr_pred_var = integration_prediction_func(test_input=data_inputs,  # curr_input,\n",
    "                                                            output_index=443, # curr_latent_index,\n",
    "                                                            my_model=my_model,\n",
    "                                                            common_background_information=common_background_information,\n",
    "                                                            pre_compute_dict=pre_compute_dict,\n",
    "                                                            config=config,\n",
    "                                                            latent_type=None, # or 'NNEncoder'\n",
    "                                                            latent_info=None)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean, all_pred_var = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=data_inputs,\n",
    "                                                        config=config,\n",
    "                                                        common_background_information=common_background_information,\n",
    "                                                        approach='integration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean4visual, all_pred_var4visual = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=inputs_total4visual,\n",
    "                                                        config=config,\n",
    "                                                        common_background_information=common_background_information,\n",
    "                                                        approach='integration',\n",
    "                                                        not4visual=False,\n",
    "                                                        n_data4visual=n_data4visual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predict = all_pred_mean[train_sample_idx_ls]\n",
    "train_rmse = (train_data_predict - data_Y_squeezed[train_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Train RMSE via integration', train_rmse)\n",
    "\n",
    "w_test_data_predict = all_pred_mean[test_sample_idx_ls]\n",
    "test_rmse = (w_test_data_predict - data_Y_squeezed[test_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Test RMSE via integration', test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nll = neg_log_likelihood(Target=data_Y_squeezed[train_sample_idx_ls], GaussianMean=all_pred_mean[train_sample_idx_ls], GaussianVar=all_pred_var[train_sample_idx_ls])\n",
    "test_nll = neg_log_likelihood(Target=data_Y_squeezed[test_sample_idx_ls], GaussianMean=all_pred_mean[test_sample_idx_ls], GaussianVar=all_pred_var[test_sample_idx_ls])\n",
    "\n",
    "print('Global Train negative log likelihood via integration:', train_nll)\n",
    "print('Global Test negative log likelihood via integration:', test_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all function index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_list = [] # list of tensors\n",
    "test_rmse_list = []\n",
    "train_nll_list = []\n",
    "test_nll_list = []\n",
    "for output_index in range(config['n_outputs']):\n",
    "    _, _, _, _, _, _, performance_dirct = evaluate_on_single_output(\n",
    "                                                        function_index = output_index,\n",
    "                                                        data_inputs = data_inputs,\n",
    "                                                        data_Y_squeezed = data_Y_squeezed,\n",
    "                                                        ls_of_ls_train_input = ls_of_ls_train_input,\n",
    "                                                        ls_of_ls_test_input = ls_of_ls_test_input,\n",
    "                                                        train_sample_idx_ls = train_sample_idx_ls,\n",
    "                                                        test_sample_idx_ls = test_sample_idx_ls,\n",
    "                                                        all_pred_mean = all_pred_mean,\n",
    "                                                        all_pred_var = all_pred_var,\n",
    "                                                        n_data4visual = n_data4visual,\n",
    "                                                        all_pred_mean4visual = all_pred_mean4visual,\n",
    "                                                        all_pred_var4visual = all_pred_var4visual                                                        \n",
    "                                                        )\n",
    "    train_rmse_list.append(performance_dirct['train_rmse'])\n",
    "    test_rmse_list.append(performance_dirct['test_rmse'])\n",
    "    train_nll_list.append(performance_dirct['train_nll'])\n",
    "    test_nll_list.append(performance_dirct['test_nll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median_index(lst):\n",
    "    sorted_lst = sorted(lst)\n",
    "    n = len(lst)\n",
    "    \n",
    "    if n % 2 != 0:\n",
    "        median = sorted_lst[n // 2]\n",
    "        return lst.index(median)\n",
    "    else:\n",
    "        mid1 = sorted_lst[n // 2 - 1]\n",
    "        mid2 = sorted_lst[n // 2]\n",
    "        \n",
    "        return lst.index(mid1)  # lst.index(mid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The output index with WORSE test rmse performance: ', test_rmse_list.index(max(test_rmse_list)))\n",
    "print('The output index with WORSE test nll performance: ', test_nll_list.index(max(test_nll_list)))\n",
    "print('------' * 10)\n",
    "print('The output index with MIDDLE test rmse performance:', find_median_index(test_rmse_list))\n",
    "print('The output index with MIDDLE test nll performance:', find_median_index(test_nll_list))\n",
    "print('------' * 10)\n",
    "print('The output index with BEST test rmse performance: ', test_rmse_list.index(min(test_rmse_list)))\n",
    "print('The output index with BEST test nll performance: ', test_nll_list.index(min(test_nll_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_index = 32\n",
    "w_train_input, w_train_target, w_test_input, w_test_target, w_gp_pred_mean, w_gp_pred_std, performance_dirct = evaluate_on_single_output(\n",
    "                                                        function_index = function_index,\n",
    "                                                        data_inputs = data_inputs,\n",
    "                                                        data_Y_squeezed = data_Y_squeezed, \n",
    "                                                        ls_of_ls_train_input = ls_of_ls_train_input,\n",
    "                                                        ls_of_ls_test_input = ls_of_ls_test_input,\n",
    "                                                        train_sample_idx_ls = train_sample_idx_ls,\n",
    "                                                        test_sample_idx_ls = test_sample_idx_ls,\n",
    "                                                        all_pred_mean = all_pred_mean,\n",
    "                                                        all_pred_var = all_pred_var,\n",
    "                                                        n_data4visual = n_data4visual,\n",
    "                                                        all_pred_mean4visual = all_pred_mean4visual,\n",
    "                                                        all_pred_var4visual = all_pred_var4visual                                                        \n",
    "                                                        )\n",
    "\n",
    "plot_traindata_testdata_fittedgp(train_X=w_train_input, train_Y=w_train_target, test_X=w_test_input, test_Y=w_test_target, gp_X=inputs_total4visual, gp_pred_mean=w_gp_pred_mean, gp_pred_std=w_gp_pred_std, inducing_points_X=my_model.variational_strategy.inducing_points_input.data, n_inducing_C=config['n_inducing_input']) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test via mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean_, all_pred_var_ = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=data_inputs,\n",
    "                                                        config=config,\n",
    "                                                        approach='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean4visual_, all_pred_var4visual_ = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=inputs_total4visual,\n",
    "                                                        config=config,\n",
    "                                                        approach='mean',\n",
    "                                                        not4visual=False,\n",
    "                                                        n_data4visual=n_data4visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predict_ = all_pred_mean_[train_sample_idx_ls]\n",
    "train_rmse_ = (train_data_predict_ - data_Y_squeezed[train_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Train RMSE via mean', train_rmse_)\n",
    "\n",
    "w_test_data_predict_ = all_pred_mean_[test_sample_idx_ls]\n",
    "test_rmse_ = (w_test_data_predict_ - data_Y_squeezed[test_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Test RMSE via mean', test_rmse_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nll_ = neg_log_likelihood(Target=data_Y_squeezed[train_sample_idx_ls], GaussianMean=all_pred_mean_[train_sample_idx_ls], GaussianVar=all_pred_var_[train_sample_idx_ls])\n",
    "test_nll_ = neg_log_likelihood(Target=data_Y_squeezed[test_sample_idx_ls], GaussianMean=all_pred_mean_[test_sample_idx_ls], GaussianVar=all_pred_var_[test_sample_idx_ls])\n",
    "\n",
    "print('Global Train negative log likelihood via mean:', train_nll_)\n",
    "print('Global Test negative log likelihood via mean:', test_nll_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all function index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_list_ = [] # list of tensors\n",
    "test_rmse_list_ = []\n",
    "train_nll_list_ = []\n",
    "test_nll_list_ = []\n",
    "for output_index in range(config['n_outputs']):\n",
    "    _, _, _, _, _, _, performance_dirct = evaluate_on_single_output(\n",
    "                                                        function_index = output_index,\n",
    "                                                        data_inputs = data_inputs,\n",
    "                                                        data_Y_squeezed = data_Y_squeezed,\n",
    "                                                        ls_of_ls_train_input = ls_of_ls_train_input,\n",
    "                                                        ls_of_ls_test_input = ls_of_ls_test_input,\n",
    "                                                        train_sample_idx_ls = train_sample_idx_ls,\n",
    "                                                        test_sample_idx_ls = test_sample_idx_ls,\n",
    "                                                        all_pred_mean = all_pred_mean_,\n",
    "                                                        all_pred_var = all_pred_var_,\n",
    "                                                        n_data4visual = n_data4visual,\n",
    "                                                        all_pred_mean4visual = all_pred_mean4visual_,\n",
    "                                                        all_pred_var4visual = all_pred_var4visual_                                                        \n",
    "                                                        )\n",
    "    train_rmse_list_.append(performance_dirct['train_rmse'])\n",
    "    test_rmse_list_.append(performance_dirct['test_rmse'])\n",
    "    train_nll_list_.append(performance_dirct['train_nll'])\n",
    "    test_nll_list_.append(performance_dirct['test_nll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median_index(lst):\n",
    "    sorted_lst = sorted(lst)\n",
    "    n = len(lst)\n",
    "    \n",
    "    if n % 2 != 0:\n",
    "        median = sorted_lst[n // 2]\n",
    "        return lst.index(median)\n",
    "    else:\n",
    "        mid1 = sorted_lst[n // 2 - 1]\n",
    "        mid2 = sorted_lst[n // 2]\n",
    "        \n",
    "        return lst.index(mid1)  # lst.index(mid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The output index with WORSE test rmse performance: ', test_rmse_list_.index(max(test_rmse_list_)))\n",
    "print('The output index with WORSE test nll performance: ', test_nll_list_.index(max(test_nll_list_)))\n",
    "print('------' * 10)\n",
    "print('The output index with MIDDLE test rmse performance:', find_median_index(test_rmse_list_))\n",
    "print('The output index with MIDDLE test nll performance:', find_median_index(test_nll_list_))\n",
    "print('------' * 10)\n",
    "print('The output index with BEST test rmse performance: ', test_rmse_list_.index(min(test_rmse_list_)))\n",
    "print('The output index with BEST test nll performance: ', test_nll_list_.index(min(test_nll_list_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_index = 2749\n",
    "w_train_input, w_train_target, w_test_input, w_test_target, w_gp_pred_mean, w_gp_pred_std, performance_dirct = evaluate_on_single_output(\n",
    "                                                        function_index = function_index,\n",
    "                                                        data_inputs = data_inputs,\n",
    "                                                        data_Y_squeezed = data_Y_squeezed, \n",
    "                                                        ls_of_ls_train_input = ls_of_ls_train_input,\n",
    "                                                        ls_of_ls_test_input = ls_of_ls_test_input,\n",
    "                                                        train_sample_idx_ls = train_sample_idx_ls,\n",
    "                                                        test_sample_idx_ls = test_sample_idx_ls,\n",
    "                                                        all_pred_mean = all_pred_mean_,\n",
    "                                                        all_pred_var = all_pred_var_,\n",
    "                                                        n_data4visual = n_data4visual,\n",
    "                                                        all_pred_mean4visual = all_pred_mean4visual_,\n",
    "                                                        all_pred_var4visual = all_pred_var4visual_                                                        \n",
    ")\n",
    "\n",
    "plot_traindata_testdata_fittedgp(train_X=w_train_input, \n",
    "                                 train_Y=w_train_target, \n",
    "                                 test_X=w_test_input, \n",
    "                                 test_Y=w_test_target, \n",
    "                                 gp_X=inputs_total4visual, \n",
    "                                 gp_pred_mean=w_gp_pred_mean, \n",
    "                                 gp_pred_std=w_gp_pred_std, \n",
    "                                 inducing_points_X=my_model.variational_strategy.inducing_points_input.data, \n",
    "                                 n_inducing_C=config['n_inducing_input'], \n",
    "                                 # title='Multi-Output Gaussian Processes, #32',\n",
    "                                 title_fontsize=15) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
