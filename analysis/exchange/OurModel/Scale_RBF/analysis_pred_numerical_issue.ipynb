{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP')\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from run_experiments.prepare_dataset import *\n",
    "from utils_general import prepare_common_background_info, pred4all_outputs_inputs, evaluate_on_single_output, plot_traindata_testdata_fittedgp, neg_log_likelihood, normalised_mean_square_error\n",
    "from code_blocks.our_models.lvmogp_svi import LVMOGP_SVI\n",
    "from code_blocks.likelihoods.gaussian_likelihood import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/configs/exchange/Scale_RBF/lvmogp_unfix.yaml'\n",
    "model_path = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/experiments_results/exchange/Scale_RBF/lvmogp_unfix/2024-03-02_21:29:20/model.pth'\n",
    "likelihood_path = '/Users/jiangxiaoyu/Desktop/All Projects/Scalable_LVMOGP/experiments_results/exchange/Scale_RBF/lvmogp_unfix/2024-03-02_21:29:20/likelihood.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_name) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "my_model = LVMOGP_SVI(\n",
    "        n_outputs = config['n_outputs'],\n",
    "        n_input = config['n_input_train'],                    # NOTE PAY ATTENTION, not total n_inputs.\n",
    "        input_dim = config['input_dim'],\n",
    "        latent_dim = config['latent_dim'],\n",
    "        n_inducing_input = config['n_inducing_input'],\n",
    "        n_inducing_latent = config['n_inducing_latent'],\n",
    "        learn_inducing_locations_latent = config['learn_inducing_locations_latent'],\n",
    "        learn_inducing_locations_input = config['learn_inducing_locations_input'],\n",
    "        latent_kernel_type = config['latent_kernel_type'],\n",
    "        input_kernel_type = config['input_kernel_type'],\n",
    "        trainable_latent_dim = config['trainable_latent_dim'] if 'trainable_latent_dim' in config else None,\n",
    "        latent_first_init = None,                               # if None, random initialization\n",
    "        latent_second_init = None,                              # if None, random initialization\n",
    "        NNEncoder = config['NNEncoder'],\n",
    "        layers = None                                           # if none, adopt default value [4, 8, 4]\n",
    "    )   \n",
    "\n",
    "my_likelihood = GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(model_path)\n",
    "my_model.load_state_dict(model_state_dict)\n",
    "\n",
    "likelihood_state_dict = torch.load(likelihood_path)\n",
    "my_likelihood.load_state_dict(likelihood_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset_type'] == 'exchange':\n",
    "    data_inputs, data_Y_squeezed, ls_of_ls_train_input, ls_of_ls_test_input, train_sample_idx_ls, test_sample_idx_ls, means, stds = prepare_exchange_data(config)\n",
    "        \n",
    "n_data4visual = 500\n",
    "inputs_total4visual = Tensor(np.linspace(2007, 2008, n_data4visual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test via Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_background_information = prepare_common_background_info(my_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore three ways to compute inverse of cholesky factor of K_uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import KroneckerProductLinearOperator, TriangularLinearOperator, LinearOperator, CholLinearOperator\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.settings import _linalg_dtype_cholesky\n",
    "\n",
    "def _cholesky_factor(induc_induc_covar: LinearOperator) -> TriangularLinearOperator:\n",
    "        L = psd_safe_cholesky(to_dense(induc_induc_covar).type(_linalg_dtype_cholesky.value()), max_tries=4)\n",
    "        return TriangularLinearOperator(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu_latent = common_background_information['K_uu_latent'].to_dense()\n",
    "K_uu_input = common_background_information['K_uu_input'].to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K_uu_input[K_uu_input < 1e-4] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu = KroneckerProductLinearOperator(K_uu_latent, K_uu_input).to_dense()\n",
    "K_uu_inv = torch.linalg.solve(K_uu, torch.eye(K_uu.size(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(torch.kron(K_uu_latent, K_uu_input), K_uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1 = torch.eye(K_uu.size(-1)) - (K_uu @ K_uu_inv)\n",
    "diff2 = torch.eye(K_uu.size(-1)) - (K_uu_inv @ K_uu)\n",
    "\n",
    "print(diff1.abs().max())\n",
    "print(diff2.abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way 1: first cholesky of each component, then inverse for each of them, finally kronecker product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_K_uu_latent = _cholesky_factor(K_uu_latent)\n",
    "chol_K_uu_input = _cholesky_factor(K_uu_input)\n",
    "chol_K_uu_latent_inv = torch.linalg.solve(chol_K_uu_latent, torch.eye(K_uu_latent.size(-1)))\n",
    "chol_K_uu_input_inv = torch.linalg.solve(chol_K_uu_input, torch.eye(K_uu_input.size(-1)))\n",
    "\n",
    "# chol_K_uu_inv_1_torch = torch.kron(chol_K_uu_latent_inv, chol_K_uu_input_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff3_1 = torch.eye(chol_K_uu_latent.size(-1)) - (chol_K_uu_latent_inv @ chol_K_uu_latent)\n",
    "diff3_2 = torch.eye(chol_K_uu_latent.size(-1)) - (chol_K_uu_latent @ chol_K_uu_latent_inv)\n",
    "\n",
    "diff4_1 = torch.eye(chol_K_uu_input.size(-1)) - (chol_K_uu_input_inv @ chol_K_uu_input)\n",
    "diff4_2 = torch.eye(chol_K_uu_input.size(-1)) - (chol_K_uu_input @ chol_K_uu_input_inv)\n",
    "\n",
    "print(diff3_1.abs().max())\n",
    "print(diff3_2.abs().max())\n",
    "print(diff4_1.abs().max())\n",
    "print(diff4_2.abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_K_uu_inv_1 = KroneckerProductLinearOperator(\n",
    "            chol_K_uu_latent_inv, chol_K_uu_input_inv) # .to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu_inv_via_chol_1 = chol_K_uu_inv_1._transpose_nonbatch() @ chol_K_uu_inv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff7_1 = torch.eye(K_uu_inv_via_chol_1.size(-1)) - K_uu_inv_via_chol_1 @ K_uu\n",
    "diff7_2 = torch.eye(K_uu_inv_via_chol_1.size(-1)) - K_uu @ K_uu_inv_via_chol_1\n",
    "\n",
    "print(diff7_1.abs().max())\n",
    "print(diff7_2.abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way2: first kronecker product, then apply cholesky factor, finally inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_K_uu = _cholesky_factor(K_uu).to_dense()\n",
    "chol_K_uu_inv_2 = torch.linalg.solve(chol_K_uu, torch.eye(K_uu.size(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu_inv_via_chol_2 = chol_K_uu_inv_2.T @ chol_K_uu_inv_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff5_1 = torch.eye(chol_K_uu.size(-1)) - chol_K_uu_inv_2 @ chol_K_uu\n",
    "diff5_2 = torch.eye(chol_K_uu.size(-1)) - chol_K_uu @ chol_K_uu_inv_2\n",
    "\n",
    "print(diff5_1.abs().max())\n",
    "print(diff5_2.abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff6_1 = torch.eye(K_uu_inv_via_chol_2.size(-1)) - K_uu_inv_via_chol_2 @ K_uu\n",
    "diff6_2 = torch.eye(K_uu_inv_via_chol_2.size(-1)) - K_uu @ K_uu_inv_via_chol_2\n",
    "\n",
    "print(diff6_1.abs().max())\n",
    "print(diff6_2.abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way3: first kronecker product, then inverse, finally cholesky factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_K_uu_inv_3 = _cholesky_factor(K_uu_inv).to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean, all_pred_var = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=data_inputs,\n",
    "                                                        config=config,\n",
    "                                                        common_background_information=common_background_information,\n",
    "                                                        approach='integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_mean4visual, all_pred_var4visual = pred4all_outputs_inputs(my_model=my_model,\n",
    "                                                        my_likelihood=my_likelihood,\n",
    "                                                        data_inputs=inputs_total4visual,\n",
    "                                                        config=config,\n",
    "                                                        common_background_information=common_background_information,\n",
    "                                                        approach='integration',\n",
    "                                                        not4visual=False,\n",
    "                                                        n_data4visual=n_data4visual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predict = all_pred_mean[train_sample_idx_ls]\n",
    "train_rmse = (train_data_predict - data_Y_squeezed[train_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Train RMSE via integration', train_rmse)\n",
    "\n",
    "w_test_data_predict = all_pred_mean[test_sample_idx_ls]\n",
    "test_rmse = (w_test_data_predict - data_Y_squeezed[test_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Test RMSE via integration', test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nll = neg_log_likelihood(Target=data_Y_squeezed[train_sample_idx_ls], GaussianMean=all_pred_mean[train_sample_idx_ls], GaussianVar=all_pred_var[train_sample_idx_ls])\n",
    "test_nll = neg_log_likelihood(Target=data_Y_squeezed[test_sample_idx_ls], GaussianMean=all_pred_mean[test_sample_idx_ls], GaussianVar=all_pred_var[test_sample_idx_ls])\n",
    "\n",
    "print('Global Train negative log likelihood via integration:', train_nll)\n",
    "print('Global Test negative log likelihood via integration:', test_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_index = 12\n",
    "w_train_input, w_train_target, w_test_input, w_test_target, w_gp_pred_mean, w_gp_pred_std, performance_dirct = evaluate_on_single_output(\n",
    "                                                        function_index = function_index,\n",
    "                                                        data_inputs = data_inputs,\n",
    "                                                        data_Y_squeezed = data_Y_squeezed, \n",
    "                                                        ls_of_ls_train_input = ls_of_ls_train_input,\n",
    "                                                        ls_of_ls_test_input = ls_of_ls_test_input,\n",
    "                                                        train_sample_idx_ls = train_sample_idx_ls,\n",
    "                                                        test_sample_idx_ls = test_sample_idx_ls,\n",
    "                                                        all_pred_mean = all_pred_mean,\n",
    "                                                        all_pred_var = all_pred_var,\n",
    "                                                        n_data4visual = n_data4visual,\n",
    "                                                        all_pred_mean4visual = all_pred_mean4visual,\n",
    "                                                        all_pred_var4visual = all_pred_var4visual                                                        \n",
    "                                                        )\n",
    "\n",
    "plot_traindata_testdata_fittedgp(train_X=w_train_input, train_Y=w_train_target, test_X=w_test_input, test_Y=w_test_target, gp_X=inputs_total4visual, gp_pred_mean=w_gp_pred_mean, gp_pred_std=w_gp_pred_std, inducing_points_X=my_model.variational_strategy.inducing_points_input.data, n_inducing_C=config['n_inducing_input']) # NOTE: input is C not X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
